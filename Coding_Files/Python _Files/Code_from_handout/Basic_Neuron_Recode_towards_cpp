#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jun 29 09:02:55 2021

@author: brahdamon
date: June 29, 2021
Location: Virtual Reality and Artificial Intelligence Laboratory, Savannah River National Laboratory, South Carolina

Recode #2 for Basic Neuron exercise, handout from Steve
We will have several functions:
1. We will have our loss function, also called our cost function. This is our objective function
2. We will have our gradient descent/ or updating function (function that makes our predictions more accurate)
3. and We will have our function for our prediction model.

"""

from random import random

# We are given two arrays, with a relationship between them.
# x = [-1, 0, 1, 2, 3, 4]
# y = [-3, -1, 1, 3, 5, 7]



xArray = [] 
yArray = []

# Create random initial values for weight and bias
init_weight = random()
init_bias = random()
weight = init_weight
bias = init_bias

partialWrtWeight = 0
partialWrtBias = 0



# Declare a learning rate
learning_rate = 0.11

# Declare a number of iterations for repetition
epochs = 100
 



# Build the Input (x) array
for i in range(6):
    xArray.append(i-1)
print(xArray)


# Build the Output (y) array
for i in xArray:
     yArray.append((2 * i) - 1)
print(yArray)





# Create a function to calculate the average
def average(numlist):
    cumSum = 0
    for num in numlist:
        cumSum += num
        
    avg = cumSum/len(numlist)
    return avg





# Create a linear Regression function
# Can be called linear regression, the neuron, the prediction model
def linReg(weight, x_i, bias):
    #y = mx + b
    prediction = []
    if(type(x_i) == list):
        for x in x_i:
            prediction.append(weight * x  + bias)
        return prediction

    else:
        print("Feature input must be of type 'list'")
        return




# Create a Function for the error
# Definition: OBJECTIVE FUNCTION: In linear programming, the function that is desired to minimize or maximize
# In this case, the objective function is the error function (want to minimize error)
def lossFunc(prediction, output):
    # Least Square Error: LSE = Sum((prediction - output)^2)
    workingSum = 0
    for output in yArray:
        workingSum += (output - prediction) ** 2
        
    error = workingSum
    return error




# Will now implement an optimization algorithm. In this case we will use a technique called
# 'gradient descent'. This is an example of a First Order Algorithm, an optimization algorithm
# usable on differentiable objective functions (which we have). 
# derivative map aka gradient descent
def gradDescent(prediction, x_i, y):
        partialWrtWeightArray = []
        partialWrtBiasArray = []
        i = 0
        partialWrtWeight = 0
        partialWrtBias = 0
    
    
        #for x in x_i:
        while i <= len(x_i):

            partialWrtWeight = 2 * ((prediction) - y[i]) * x_i
            partialWrtBias = 2 * ((prediction) - y[i])
            i += 1
    
            partialWrtWeightArray.append(partialWrtWeight)
            partialWrtBiasArray.append(partialWrtBias)

        return partialWrtWeightArray, partialWrtBiasArray




for e in range(epochs):
    prediction = linReg(weight, xArray, bias)
    
    avgPrediction = average(prediction)
    

    partialWrtWeightArray, partialWrtBiasArray = gradDescent(avgPrediction, xArray, yArray)
    
    avgPartialWrtWeight = average(partialWrtWeightArray)
    avgPartialWrtBias = average(partialWrtBiasArray)



# We want to update our values for weight and bias such that 
# the error function trends toward minimization (is eventually minimized)
# The partial derivatives can be large integer values, so we want to scale the adjustments
# by multiplication by a learning_rate.
    
    
    weight = weight - partialWrtWeight * learning_rate
    bias = bias - partialWrtBias * learning_rate

    avgWeight = average(weight)
    avgBias = average(bias)


    print(avgWeight)
    print(avgBias)
